#+AUTHOR:      Arnaud Legrand
#+TITLE:       Introduction to Probabilities and Statistics
#+DATE:        MOSIG Lecture
#+STARTUP: beamer overview indent
#+TAGS: noexport(n)
#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [11pt,xcolor=dvipsnames,presentation]
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:nil -:t f:t *:t <:t
#+LATEX_HEADER: \input{org-babel-style-preembule.tex}
#+LATEX_HEADER: \usepackage{commath}

#+LaTeX: \input{org-babel-document-preembule.tex}


#+BEGIN_LaTeX
\def\R{\ensuremath{\mathbb{R}}\xspace}
\def\F{\ensuremath{\mathcal{F}}\xspace}
\def\N{\ensuremath{\mathcal{N}}\xspace}
\def\P{\ensuremath{\operatorname{P}}\xspace}
\def\E{\ensuremath{\operatorname{E}}\xspace}
\def\Var{\ensuremath{\operatorname{Var}}\xspace}
\def\rv#1{\ensuremath{\textcolor{DarkBlue}{#1}}\xspace}

\newcommand\distrib[1]{%
  \includegraphics[width=.142\linewidth,page=1]{#1.pdf}&%
  \includegraphics[width=.142\linewidth,page=2]{#1.pdf}&%
  \includegraphics[width=.142\linewidth,page=3]{#1.pdf}&%
  \includegraphics[width=.142\linewidth,page=4]{#1.pdf}&%
  \includegraphics[width=.142\linewidth,page=5]{#1.pdf}&%
  \includegraphics[width=.142\linewidth,page=6]{#1.pdf}&%
  \includegraphics[width=.142\linewidth,page=7]{#1.pdf}
}
#+END_LaTeX
* List                                                             :noexport:
** TODO Translate to org
** TODO Redo all figures
** TODO Reorder (talk about median, min, etc earlier)
* A probabilistic model
** 
*** Continuous random variable
- A *random variable* (or stochastic variable) is, roughly speaking, a
  variable whose value results from a measurement

  Such a variable enables to model *uncertainty* that may result of
  *incomplete information* or *imprecise measurements*

  You can think of it as a /box/. Every time you open the box, you get a
  different value.
- Formally $(\Omega, \F, \P)$ is a probability space where:
  - $\Omega$, the sample space, is the set of all possible 
    #+LaTeX: outcomes (\eg $\{Head,Tail,$ $Edge,Lost\}$)
  - \F if the set of events where an event is a set containing zero or
    more outcomes (\eg the event of "making a coin toss decision"
    $\{Head,Tail\}$)
  - The probability measure $\P:\F \to [0,1]$ is a function returning an
    event's probability
- A random variable associates a numerical value to outcomes.  Since
  many computer science experiments are based on time measurements, we
  focus on *continuous* variables.
  \begin{equation*}
  \rv{X}: \Omega \to \R
  \end{equation*}
*** Probability distribution
A *probability distribution* (a.k.a. *probability density function* or
p.d.f.) is used to describe the probabilities of different *values*
occurring

A random variable \rv{X} has density $f$, where $f$ is a non-negative and
integrable function, if:
#+BEGIN_LaTeX
\begin{equation*}
\P[a \leq \rv{X} \leq b] = \int_a^b f(x) \, \dif x
\end{equation*}
#+END_LaTeX

#+begin_src R :results output graphics :file "pdf_babel/Gamma_distribution.pdf" :exports none :width 6 :height 3 :session
library(ggplot2)
library(ggthemes)
df = data.frame(x=c(-2,10), y=c(0,.3))
func = dgamma
pfunc = pgamma
args = list(shape = 3)
xmin = 1
xmax = 6
x = seq(from=xmin,to=xmax,length.out=50)
y = do.call(func,c(list(x=x),args))
area = data.frame(x=x, y=y)
integral = diff(range(do.call(pfunc,c(list(q=c(xmin,xmax)),args))))
label = paste("P(paste(",xmin," <= X) <= ",xmax,") == ", integral)

p = ggplot(data=df,aes(x=x,y=y)) + geom_point(size=0) + theme_classic() + 
    stat_function(fun = func, colour = "darkgreen", arg = args) +
        geom_area(data=area,aes(x=x,y=y),fill="lightskyblue2") + 
            geom_text(x=.7*max(df$x),y=.7*max(df$y), label=label, parse=T) +
            ylab("f(x)") + xlim(df$x)
p
# ggsave(p,file="pdf_babel/Gamma_distribution.pdf",width=6,height=4)
#+end_src

#+RESULTS:
[[file:pdf_babel/Gamma_distribution.pdf]]

#+BEGIN_CENTER
\includegraphics[width=.8\linewidth]{pdf_babel/Gamma_distribution.pdf}
#+END_CENTER
*** Characterizing a random variable
The probability density function fully characterizes the random
variable but it is complex object. 

- It may have one or several *modes*
- It may have a bounded support or not, hence the random variable may
  have a *minimal* and/or a *maximal* value
- The *median* cuts the probabilities in half
- It may be symmetrical or not

#+begin_src R :results output graphics :file "pdf_babel/distribution_characteristics.pdf" :exports none :width 6 :height 3 :session
library(ggplot2)
library(ggthemes)
xmin = -2
xmax = 10
ymin = 0
ymax = .3

func = dgamma
pfunc = pgamma
args = list(shape = 3)

x = seq(from=xmin,to=xmax,length.out=500)
y = do.call(pfunc,c(list(q=x),args))
dfminx = data.frame(x=x,y=y)
minx = tail(dfminx[dfminx$y==0,],n=1)$x
if(length(minx)==0) {minx=NA}
maxx = head(dfminx[dfminx$y==1,],n=1)$x
if(length(maxx)==0) {maxx=NA}
medianx = tail(dfminx[dfminx$y<.5,],n=1)$x
if(length(medianx)==0) {medianx=NA}

y = do.call(func,c(list(x=x),args))
dfminx = data.frame(x=x,y=y)
modex = dfminx[dfminx$y==max(dfminx$y),]$x
espx = sum(dfminx$x*dfminx$y)*diff(range(head(dfminx$x,n=2)))

dfstat = data.frame(name = c("min", "median", "max","mode","expected value"),
                    x = c(minx,medianx,maxx,modex,espx),
                    y = ymax)

p = ggplot(data=dfstat,aes(x=x,y=y,color=name)) + geom_line(alpha=1) +
    xlim(xmin,xmax) + ylim(ymin,ymax) + theme_classic() + ylab("f(x)") +
    stat_function(fun = func, colour = "black", arg = args) +
    geom_vline(aes(xintercept=x,color=name)) + guides(colour = guide_legend(""))

p
# ggsave(p,file="pdf_babel/Gamma_distribution.pdf",width=6,height=4)
#+end_src

#+RESULTS:
[[file:pdf_babel/distribution_characteristics.pdf]]

#+BEGIN_CENTER
\includegraphics[width=.8\linewidth]{pdf_babel/distribution_characteristics.pdf}
#+END_CENTER
*** Expected value
- When one speaks of the "expected price", "expected height", etc. one
  means the *expected value* of a random variable that is a price, a
  height, etc.o
  \begin{align*}
  \E[\rv{X}] & = x_1p_1 + x_2p_2 + \ldots + x_kp_k = \int_{-\infty}^\infty x f(x)\, \dif x
  \end{align*}
  The expected value of \rv{X} is the ``average value'' of \rv{X}.\smallskip

  It is \textbf{not} the most probable value. The mean is _one_ aspect
  of the distribution of \rv{X}. The *median* or the *mode* are other
  interesting aspects.
- The *variance* is a measure of how far the values of a
  random variable are spread out from each other.

  If a random variable \rv{X} has the expected value (mean) $\mu =
  \E[\rv{X}]$, then the variance of \rv{X} is given by:
  #+BEGIN_LaTeX
  \begin{align*} 
      \Var(\rv{X}) &= \E\left[(\rv{X} - \mu)^2
      \right] = \int_{-\infty}^\infty  (x-\mu)^2 f(x)\, \dif x
  \end{align*}
  #+END_LaTeX
- The *standard deviation $\sigma$* is the square root of the variance. This
  normalization allows to compare it with the expected value
* Estimation
** 
*** How to estimate the Expected value ?
To empirically *estimate* the expected value of a random variable
\rv{X}, one repeatedly measures observations of the variable and
computes the arithmetic mean of the results \bigskip This is called
the *sample mean*

Unfortunately, if you repeat the estimation, you may get a different
value since \rv{X} is a random variable \dots
*** Central Limit Theorem
- Let $\{\rv{X_1}, \rv{X_2}, \dots, \rv{X_n}\}$ be a random sample of size
  $n$ (\ie a sequence of \textbf{independent} and \textbf{identically
  distributed} random variables with expected values $\mu$ and variances
  $\sigma^2$)
- The sample average of these random variables is:
  #+BEGIN_LaTeX
  \begin{equation*}
  \rv{S_n} =  \frac{1}{n} (\rv{X_1} + \dots + \rv{X_n})
  \end{equation*}  
  #+END_LaTeX
  $\rv{S_n}$ is a random variable too!
- For large n's, the distribution of $\rv{S_n}$ is approximately
  normal with mean $\mu$ and variance $\frac{\sigma^2}{n}$
  #+BEGIN_LaTeX
  \begin{equation*}
  \rv{S_n} \xrightarrow[n\to\infty]{} \N\left(\mu,\frac{\sigma^2}{n}\right)
  \end{equation*}
  #+END_LaTeX
*** The Normal Distribution
#+begin_src R :results output graphics :file "pdf_babel/normal_distribution.pdf" :exports none :width 6 :height 2.5 :session
library(ggplot2)
library(ggthemes)
xmin = -5
xmax = 5
ymin = 0
ymax = 1.5

dfnorm = data.frame(mu=c(0,0,0,-2),sigma2=c(.1,1,5,.5))
dfnorm$label = paste0("mu=",dfnorm$mu,", sigma^2=",dfnorm$sigma2)

df = data.frame(x=c(xmin,xmax),y=c(ymin,ymax))

p = ggplot(data=df,aes(x=x,y=y)) + 
    xlim(xmin,xmax) + ylim(ymin,ymax) + theme_classic() +
    guides(colour = guide_legend("")) + ylab("f(x)")

## Argh, this does not work either. I have to do it "manually". :(
# for(i in 1:dim(dfnorm)[1]) {
#   d = dfnorm[i,]
#   print(d$label)
#   p = p + stat_function(fun = dnorm, 
#                         arg = list(mean=d$mu, sd=d$sigma2), 
#                         aes(color=dfnorm[i,]$label))
# }

p + stat_function(fun = dnorm, arg = list(mean=dfnorm[1,]$mu, sd=sqrt(dfnorm[1,]$sigma2)), aes(color=dfnorm[1,]$label)) +
    stat_function(fun = dnorm, arg = list(mean=dfnorm[2,]$mu, sd=sqrt(dfnorm[2,]$sigma2)), aes(color=dfnorm[2,]$label)) +
    stat_function(fun = dnorm, arg = list(mean=dfnorm[3,]$mu, sd=sqrt(dfnorm[3,]$sigma2)), aes(color=dfnorm[3,]$label)) +
    stat_function(fun = dnorm, arg = list(mean=dfnorm[4,]$mu, sd=sqrt(dfnorm[4,]$sigma2)), aes(color=dfnorm[4,]$label))
#+end_src

#+RESULTS:
[[file:pdf_babel/normal_distribution.pdf]]

#+BEGIN_LaTeX
  \begin{overlayarea}{\linewidth}{4.5cm}
    \begin{center}%
      \includegraphics<1>[height=4.5cm]{pdf_babel/normal_distribution.pdf}%
      \includegraphics<2>[height=4.5cm]{images/Standard_deviation_diagram.pdf}%
    \end{center}
  \end{overlayarea}
  \uncover<1->{The smaller the variance the more ``spiky'' the
    distribution.}
  \uncover<2->{
#+END_LaTeX
- Dark blue is less than one standard deviation from the mean. For the
  normal distribution, this accounts for about 68% of the set.
- Two standard deviations from the mean (medium and dark blue) account
  for about 95%
- Three standard deviations (light, medium, and dark blue) account for
  about 99.7%
#+LaTeX: }
*** CLT Illustration
#+begin_src R :results output graphics :file "pdf_babel/CLT_illustration.pdf" :exports none :width 9 :height 6 :session
library(ggplot2)
library(ggthemes)

triangle <- function(n=10) {
  sqrt(runif(n)) 
}

broken <- function(n=10) {
  x=runif(n);
  x/(1-x);
}

broken_mid <- function(n=10) {
  x=(runif(n)+runif(n))/2;
  x/(1-x);
}


generate <- function(n=50000,N=c(1,2,5,10,15,20,30,100), law=c("unif","binom","triangle")) {
  df=data.frame();
  for(l in law) {
    for(p in N) {
      X=rep.int(0,n);
      for(i in 1:p) {
        X = X + switch(l, unif = runif(n),
                          binom = rbinom(n,1,.5), 
                          exp=rexp(n,rate = 2), 
                          norm=rnorm(n,mean = .5),
                          triangle=triangle(n)-1/6,
                          broken=broken(n),
                          broken_mid=broken_mid(n));
      }
      X = X/p;
      df=rbind(df,data.frame(N=p,SN=X,law=l));
    }
  } 
  df;
}
d=generate()
ggplot(data=d,aes(x=SN)) + geom_density(aes(y = ..density..)) + 
     facet_grid(law~N) + theme_classic() + xlab("") + 
     scale_x_continuous(breaks=c(0,.5,1))
#+end_src

#+RESULTS:
[[file:pdf_babel/CLT_illustration.pdf]]

  
Start with an arbitrary distribution and compute the distribution of
$S_n$ for increasing values of $n$.
#+BEGIN_CENTER
#+LaTeX: \includegraphics<1>[width=.8\linewidth]{pdf_babel/CLT_illustration.pdf}
#+END_CENTER

*** CLT consequence: confidence interval
#+begin_src R :results output graphics :file pdf_babel/CI_illustration.pdf :exports none :width 5 :height 3 :session
mu = 500
N = 30
n = 40
X = 0
for (i in 1:N) {
    X = X + mu + runif(n, min = -1, max = 1) # Hence var=1/3
}
# so sigma_n = sqrt(1/3)/sqrt(N)
ci = 2*sqrt(1/3)/sqrt(N);

X = X/N

# length(X[X >= 1775.5 & X <= 1776.6])/length(X)

df = data.frame(x = X, y = seq(1:length(X)))
df$valid = 1
df[abs(df$x - mu) > ci, ]$valid = 0
ggplot(df, aes(x = x, y = y, color = factor(valid))) + geom_point() + 
    geom_errorbarh(aes(xmax = x - ci, xmin = x + ci)) + 
    geom_vline(xintercept = mu) + 
    theme_classic() + guides(colour = guide_legend("")) +
    xlim(mu-3*ci,mu+3*ci) + 
    ylab("Trial #") + xlab("Observed value")
#+end_src

#+RESULTS:
[[file:pdf_babel/CI_illustration.pdf]]

#+BEGIN_LaTeX
\begin{overlayarea}{\linewidth}{4.5cm}
  \begin{center}%
    \includegraphics<1>[height=4.5cm]{images/Standard_deviation_diagram.pdf}%
    \includegraphics<2>[height=4.5cm]{pdf_babel/CI_illustration.pdf}%
  \end{center}
\end{overlayarea}
#+END_LaTeX

When $n$ is large:
#+BEGIN_LaTeX
\begin{center}
  \scalebox{.9}{$\displaystyle
  \P\left(\mu\in
    \left[\rv{S_n}-2\frac{\sigma}{\sqrt{n}},\rv{S_n}+2\frac{\sigma}{\sqrt{n}}\right]\right)
  = \P\left(\rv{S_n}\in
    \left[\mu-2\frac{\sigma}{\sqrt{n}},\mu+2\frac{\sigma}{\sqrt{n}}\right]\right)
  \approx  95\%$}
\end{center}
\uncover<2>{There is 95\% of chance that the \alert{true mean} lies
  within 2$\frac{\sigma}{\sqrt{n}}$ of the \alert{sample mean}.}
#+END_LaTeX
* Using Confidence Intervals                                       :noexport:
** Comparing Two Alternatives
  Assume, you have evaluated two scheduling heuristics $A$ and $B$ on
  $n$ different DAGs.
  \begin{center}
    \begin{overlayarea}{.9\linewidth}{4.5cm}
      \begin{center}%
        \includegraphics<1>[scale=.911,subfig=1]{2sample_comp.fig}%
        \includegraphics<2>[scale=.911,subfig=2]{2sample_comp.fig}%
        \includegraphics<3->[scale=.911,subfig=3]{2sample_comp.fig}%
      \end{center}
    \end{overlayarea}
  \end{center}
    \begin{overlayarea}{\linewidth}{1.5cm}%
      \only<1>{The two 95\% confidence intervals do not overlap
        $\leadsto$ $\P(\mu_A<\mu_B)>90\%$.\\}%
      \only<2>{The two 95\% confidence intervals do overlap
        $\leadsto$ ??.% \\
        \begin{flushright}
          Reduce C.I ?
        \end{flushright}
      }%
      \only<3>{The two 70\% confidence intervals do not overlap
        $\leadsto$ $\P(\mu_A<\mu_B)>49\%$.%\\
        \begin{flushright}
          Let's do more experiments instead.
        \end{flushright}
      }%
      \only<4->{The width of the confidence interval is proportional
        to $\frac{\sigma}{\sqrt{n}}$.
        \begin{flushright}
          Halving C.I. requires 4 times more experiments!
        \end{flushright}
        Try to \concept{reduce variance} if you can...
      }
    \end{overlayarea}

** Comparing Two Alternatives with Blocking
  \begin{itemize}
  \item C.I.s overlap because variance is large. Some DAGS have an
    intrinsically longer makespan than others, hence a large
    $\Var(A)$ and $\Var(B)$
    \begin{center}
      \includegraphics<1>[scale=.7,subfig=2]{2sample_comp.fig}%
      \includegraphics<2->[scale=.7,subfig=4]{2sample_comp.fig}%
    \end{center}
  \item<2-> The previous test estimates $\mu_A$ and $\mu_B$
    \concept{independently}.

    $\E[A]<\E[B] \Leftrightarrow \E[B-A]<0$. 

    In the previous evaluation, the \concept{same} DAG is used for
    measuring $A_i$ and $B_i$, hence we can focus on $B-A$.

    Since $\Var(B-A)$ is much smaller than $\Var(A)$ and $\Var(B)$, we
    can conclude that $\mu_A<\mu_B$ with 95\% of confidence.

  \item<3-> Relying on such common points is called \concept{blocking}
    and enable to \concept{reduce variance}.
  \end{itemize}

** How Many Replicates ?
  \begin{itemize}
  \item<+-> The CLT says that ``when $n$ goes large'', the sample mean
    is normally distributed.

    The CLT uses $\sigma = \sqrt{\Var(X)}$ but we only have the
    sample variance, not the true variance.\\[0cm]
    \begin{overlayarea}{\linewidth}{0cm}
      \vspace{-1.3cm}
      \begin{center}
        \includegraphics<+>[width=.7\linewidth]{sample_var.pdf}
      \end{center}
    \end{overlayarea}
    \uncover<+->{\textbf{Q:} How Many Replicates ?} \\
    ~\hfill\uncover<+->{\textbf{A1:} How many can you afford ?}\\
    ~\hfill\uncover<+->{\textbf{A2:} 30\dots\hspace{3.25cm}~\\
    \textbf{Rule of thumb:} a sample of 30 or more is big sample but a
    sample of 30 or less is a small one (doesn't always work).}
  \item<+-> With less than 30, you need to make the $C.I.$ wider using
    e.g. the \concept{Student law}.
  \item<+-> Once you have a first C.I. with 30 samples, you can estimate
    how many samples will be required to answer your question. If it
    is too large, then either try to reduce variance (or the scope of
    your experiments) or simply explain that the two alternatives are
    hardly distinguishable...
  \item<+-> \textbf{Running the right number of experiments enables to
      get to conclusions more quickly and hence to test other
      hypothesis.}
  \end{itemize}

** Key Hypothesis
  The hypothesis of CLT are very weak. Yet, to qualify as replicates,
  the repeated measurements:
  \begin{itemize}
  \item must be independent (take care of warm-up)
  \item must not be part of a time series (the system behavior may
    temporary change)
  \item must not come from the same place (the machine may have a problem)
  \item must be of appropriate spatial scale
  \end{itemize}
  \begin{center}
    \textbf{Perform graphical checks}
  \end{center}

** Simple Graphical Check
  \begin{center}
    \includegraphics<1>[width=.45\linewidth]{4-plot-1.jpg}%
    \includegraphics<2>[width=.45\linewidth]{4-plot-2.jpg}
  \end{center}
  \hspace{-.05\linewidth}
  \begin{minipage}{1.1\linewidth}
    \small
    \vspace{-.5em}
    \begin{description}
    \item[Fixed Location:] If the fixed location assumption holds,
      then the run sequence plot will be flat and non-drifting.\vspace{-.5em}
    \item[Fixed Variation:] If the fixed variation assumption holds,
      then the vertical spread in the run sequence plot will be the
      approximately the same over the entire horizontal axis.\vspace{-.5em}
    \item[Independence:] If the randomness assumption holds, then the
      lag plot will be structureless and random.\vspace{-.5em}
    \item[Fixed Distribution]: If the fixed distribution assumption
      holds, in particular if the fixed normal distribution holds,
      then\vspace{-.5em}
      \begin{itemize}
      \item the histogram will be bell-shaped, and
      \item the normal probability plot will be linear.
      \end{itemize}
      If you see several modes, you may want to investigate further is
      there is not another hidden parameter you should take into account.
    \end{description}
  \end{minipage}

** Temporal Dependancy
  \begin{center}
    \includegraphics<1>[width=\linewidth,height=4cm]{simul/deptempo.pdf}%
    \includegraphics<2>[width=\linewidth,height=4cm]{simul/deptempo-zoom.pdf}%
  \end{center}
  \begin{itemize}
  \item Looks independant and statistically identical
  \item<2> Danger: temporal correlation $\leadsto$ study stationnarity. 
  \end{itemize}

** Detect Trends
  \begin{center}
    \includegraphics[width=\linewidth,height=4cm]{simul/unifderiv.pdf}%
  \end{center}
  \begin{itemize}
  \item Model the trend: here increase then saturates
  \item Possibly remove the trend by compensating it (multiplicative
    factor here)
  \end{itemize}

** Detect Periodicity
  \begin{center}
    \includegraphics[width=\linewidth,height=4cm]{simul/periode.pdf}%
  \end{center}
  May depend on sampling frequency or on horloge resolution.
  \begin{itemize}
  \item Study the period (Fourier)
  \item Use time series
  \end{itemize}
** Confidence...
#+BEGIN_LaTeX
\begin{center}
  \begin{tabular}{c|c}
    \includegraphics[scale=.32]{images/xkcd_significant_1.png}&
    \includegraphics[scale=.32]{images/xkcd_significant_2.png}
  \end{tabular}
\end{center}
#+END_LaTeX

* Design of Experiments: Early Intuition                           :noexport:

** Comparing Two Alternatives (Blocking + Randomization)
  \begin{itemize}[<+->]
  \item When comparing A and B for different settings, doing $A, A, A,
    A, A, A$ and then $B, B, B, B, B, B$ is a bad idea.
  \item You should better do $A, B, \quad A, B,\quad A, B,\quad A, B,
    \dots $.
  \item Even better, randomize your run order. You should flip a coin
    for each configuration and start with A on head and with B on
    tail...
    \begin{center}
      $A, B,\quad B, A,\quad  B, A,\quad A, B, \dots $.
    \end{center}
    With such design, you will even be able to check whether being the
    first alternative to run changes something or not.
  \item Each configuration you test should be run on different
    machines.
    
    You should record as much information as you can on how the
    experiments was performed (\url{http://expo.gforge.inria.fr/}).
  \end{itemize}

** Experimental Design
  There are two key concepts:
  \begin{center}
    \concept{replication} and \concept{randomization}
  \end{center}
  You replicate to \concept{increase reliability}. You randomize to
  \concept{reduce bias}.

  \begin{center}
    \textbf{    If you replicate thoroughly and randomize properly, \\
      you will not go far wrong.  }  \end{center} 
  \pause
  \begin{quote}\sf
    It doesn't matter if you cannot do your own advanced statistical
    analysis. If you designed your experiments properly, you may be
    able to find somebody to help you with the statistics.\smallskip

    If your experiments is not properly designed, then no matter how
    good you are at statistics, you experimental effort will have been
    wasted.
  \end{quote}\vspace{-1em}
  \begin{center}
    \textbf{No amount of high-powered statistical analysis can turn a
      bad experiment into a good one.}
  \end{center}
% Experimental Design (replication and randomization)
% - general recommendations (parsimony, true randomization)
% - replication vs. pseudo-replication
% - Strong inference vs. weak inference

* Getting rid of Outliers                                          :noexport:

** Abnormal measurements
  \begin{center}
    \includegraphics<1>[width=\linewidth,height=3cm]{simul/cauchy1.pdf}%
  \end{center}
  \begin{itemize}
  \item Rare events: interpretation
  \item Get rid of it using:
    \begin{itemize}
    \item a threshold value: what is the right threshold ?
    \item quantiles: what is the good rejection rate ?
    \end{itemize}
  \end{itemize}

** Thresholds
  \begin{center}
    Reject values larger than 10 $\leadsto$ 5\% of rejection\\
    \includegraphics<1>[width=\linewidth,height=3cm]{simul/cauchy-seuil10.pdf}%
  \end{center}


  \begin{center}
    Reject values larger than 50 $\leadsto$ 1\% of rejection\\
    \includegraphics<1>[width=\linewidth,height=3cm]{simul/cauchy-seuil1pc.pdf}%
  \end{center}

  Actually, here, the samples are generated using the Cauchy
  distribution, which is pathological for most ideas you may come up
  with. :)

* Issues when studying something else than the mean                :noexport:
** Summarizing the distribution
  \begin{center}
    \includegraphics[width=\linewidth,height=3cm]{simul/histogramme2.pdf}%
  \end{center}
  What is the shape of the histogram:
  \begin{itemize}
  \item uni/multi-modal
  \item symmetrical or not ($\leadsto$ skewness)
  \item Flat of not ($\leadsto$ kurtosis)
  \end{itemize}
  Summarize with \concept{central tendancy}

** Summarizing the distribution
  \begin{center}
    \includegraphics[width=\linewidth,height=3cm]{simul/histogramme2.pdf}%
  \end{center}
  \begin{itemize}
  \item Mode: the most probable value (higly depends on the
    bin size)
  \item Median: splits the samples in half (rather unstable)
  \item Mean: average ``cost'' (can simply estimate confidence intervals)
  \end{itemize}

** Mode value
{\bf histogram}
\begin{center}
\includegraphics[width=4.6cm]{simul/histogramme.pdf}
\end{center}
\begin{block}{Mode}
\begin{itemize}
\item  {\green{\bf Categorical data}}
\item Most frequent value
\item highly unstable value
\item for continuous value distribution depends on the histogram step
\item interpretation depends on the flatness of the histogram
\end{itemize}
\alert{\bf $\Longrightarrow$ Use it carefully} 

\alert{\bf $\Longrightarrow$ Predictor function} 

\end{block}

** Median value
{\bf histogram}
\begin{center}
%\includegraphics[width=4.6cm]{histogramme.pdf}
\end{center}
\begin{block}{Median}
\begin{itemize}
\item {\green{\bf Ordered data}}
\item Split the sample in two equal parts 
\[
\sum_{i\leq Median}f_i \leq \frac 1 2 \leq \sum_{i\leq Median+1}f_i .\]
\item more stable value
\item does not depends on the histogram step 
\item difficult to combine (two samples)
\end{itemize}


\alert{\bf $\Longrightarrow$ Randomized algorithms} 

\end{block}

** Mean value
{\bf histogram}
\begin{center}
%\includegraphics[width=4.6cm]{histogramme.pdf}
\end{center}
\begin{block}{Mean}
\begin{itemize}
\item {\green{\bf Vector space}}
\item Average of values 
\[
Mean = \frac 1 {Sample\_Size}\sum x_i = \sum_{x}x.f_x .\]
\item stable value 
\item does not depends on the histogram step 
\item easy to combine (two samples $\Rightarrow$ weighted mean)
\end{itemize}


\alert{\bf $\Longrightarrow$ Additive problems (cost, durations, length,...)} 

\end{block}

** Central tendency
{\bf histogram}
\begin{center}
\includegraphics[width=4.6cm]{simul/histogramme2.pdf}
\end{center}
\begin{block}{Complementarity}
\begin{itemize}
\item Valid if the sample is "Well-formed"
\item {\green{\bf Semantic of the observation}}
\item Goal of analysis 
\end{itemize}

\alert{\bf $\Longrightarrow$ Additive problems (cost, durations, length,...)} 

\end{block}

** Central tendency (2)
\begin{block}{Summary of Means}
\begin{itemize}
\item Avoid means if possible\\
Loses information
\item  {\blue Arithmetic mean}\\
 When sum of raw values has physical meaning\\
Use for summarizing times (not rates)
\item {\blue Harmonic mean}\\
Use for summarizing rates (not times)
\item {\blue Geometric mean}\\
 Not useful when time is best measure of perf\\
 Useful when multiplicative effects are in play
\end{itemize}
\end{block}

** Computational aspects
\begin{itemize}
\item Mode : computation of the histogram steps, then computation of max  $O(n)$  ``off-line''
\item Median : sort the sample $O(nlog(n))$ or $O(n)$ (subtile algorithm) ``off-line''
\item Mean : sum values $O(n)$ ``on-line'' computation
\end{itemize}
\pause
\begin{center}
\alert{\large \bf Is the central tendency significant ? \\
$\Rightarrow$ Explain variability.}
\end{center}

** Variability
\begin{block}{Categorical data (finite set)}
$f_i$ : empirical frequency of element $i$

Empirical entropy
\[
H(f)=\sum_i f_i \log f_i.\]
Measure the empirical distance with the uniform distribution
\begin{itemize}
\item $H(f)\geq 0$
\item $H(f)=0$ iff the observations are reduced to a unique value
\item $H(f)$ is maximal for the uniform distribution
\end{itemize} 
\end{block}
\begin{frame}{Variability (2)}
\begin{block}{Ordered data }

Quantiles : quartiles, deciles, etc

Sort the sample : 
\[
(x_1,x_2,\cdots ,x_n)\longrightarrow  (x_{(1)},x_{(2)},\cdots ,x_{(n)});\]
\[
Q_1=x_{(n/4)};\; Q_2=x_{(n/2)}=Median;\; Q_3=x_{(3n/4)}.\]

For deciles
\[
d_i = argmax_i \{\sum_{j\leq i}f_j \leq \frac i {10}\}.\]
Utilization as quantile/quantile plots to compare distributions
\end{block}
\begin{frame}{Variability (3)}
\begin{block}{ Vectorial data }

Quadratic error for the mean 
\[
Var(X)=\frac 1 n \sum_1^n (x_i-\bar{x}_n)^2.\]
{\bf Properties:}
\begin{eqnarray*}
Var(X) & \geq & 0;\\
Var(X)&=&\overline{x^2}-(\bar{x})^2, \;\;\mbox{o\`u} \;\;\overline{x^2}=\frac 1 n \sum_{i=1}^n x_i^2.\;\\
Var(X+cste)&=&Var(X);\\
Var(\lambda X)&=&\lambda^2 Var(X).
\end{eqnarray*}

\end{block}

** Roadmap for a good data analysis
\begin{enumerate}
\item Plot the sample (various representations)
\item Describe the results (data analysis)
\item Preliminary processing : remove or flag outliers, estimate or flag  missing values
\item Propose a stochastic model : establish the hypothesis : independence (time correlation, auto-correlation), stationarity, same probability law
\item Summarize data by a  histogram
\item Comment the shape (modal/skewness/flatness/...)
\item Estimate the central tendency of the sample : choose the central index
\item Estimate the accuracy of the result (confidence intervals)
\item Propose a visualization
\end{enumerate}

** References
   \nocite{Jain1991,Lilja200912,Ross201002,Montgomery200901}
   \bibliographystyle{plain}
   \bibliography{biblio}



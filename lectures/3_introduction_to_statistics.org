#+AUTHOR:      Arnaud Legrand
#+TITLE:       Introduction to Probabilities and Statistics
#+DATE:        MOSIG Lecture
#+STARTUP: beamer overview indent
#+TAGS: noexport(n)
#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [11pt,xcolor=dvipsnames,presentation]
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:nil -:t f:t *:t <:t
#+LATEX_HEADER: \input{org-babel-style-preembule.tex}
#+LATEX_HEADER: \usepackage{commath}

#+LaTeX: \input{org-babel-document-preembule.tex}


#+BEGIN_LaTeX
\def\R{\ensuremath{\mathbb{R}}\xspace}
\def\F{\ensuremath{\mathcal{F}}\xspace}
\def\N{\ensuremath{\mathcal{N}}\xspace}
\def\P{\ensuremath{\operatorname{P}}\xspace}
\def\E{\ensuremath{\operatorname{E}}\xspace}
\def\Var{\ensuremath{\operatorname{Var}}\xspace}
\def\rv#1{\ensuremath{\textcolor{blue}{#1}}\xspace} % DarkBlue

\newcommand\distrib[1]{%
  \includegraphics[width=.142\linewidth,page=1]{#1.pdf}&%
  \includegraphics[width=.142\linewidth,page=2]{#1.pdf}&%
  \includegraphics[width=.142\linewidth,page=3]{#1.pdf}&%
  \includegraphics[width=.142\linewidth,page=4]{#1.pdf}&%
  \includegraphics[width=.142\linewidth,page=5]{#1.pdf}&%
  \includegraphics[width=.142\linewidth,page=6]{#1.pdf}&%
  \includegraphics[width=.142\linewidth,page=7]{#1.pdf}
}
#+END_LaTeX
* List                                                             :noexport:
** TODO Translate to org
** TODO Redo all figures
** TODO Reorder (talk about median, min, etc earlier)
* A (mathematical) probabilistic model
** 
*** Continuous random variable (1/2)
\vspace{-.3em}
- A *random variable* (or stochastic variable) is, roughly speaking, a
  variable whose value results from a measurement (or an observation)

  Such a variable enables to model *uncertainty* that may result of
  *incomplete information* or *imprecise measurements* \pause

  You can think of it as a *small box*:
  - Every time you open the box, you get a _different_ value.
  - I will use this box analogy throughout the whole lecture and I
    encourage you to ask yourself what the box can be in your own
    studies\medskip
  \pause
- Formally a *probability space* is defined by $(\Omega, \F, \P)$ where:
  - $\Omega$, the *sample space*, is the set of all possible *outcomes*
    - \Eg
      all the possible combinations of your DNA with the one of your
      {girl|boy}friend
    - You may or may not be able to observe directly the outcome.
    \pause
  - \F if the set of *events* where an event is a set containing zero or
    more outcomes
    - \Eg the event of "the DNA corresponds to a girl with blue eyes"
    - An event is somehow more tangible and can generally be observed
    \pause
  - The *probability measure* $\P:\F \to [0,1]$ is a function returning an
    event's probability 
    #+LaTeX: ($\P$("having a brown-eyed baby girl") = 0.0005)
*** Continuous random variable (1/2)
- A *random variable* associates a *numerical value* to *outcomes*
  \begin{equation*}
  \rv{X}: \Omega \to \R
  \end{equation*}
  \vspace{-1.6em}
  - \Eg the weight of the baby at birth (assuming it solely depends
    on DNA, which is quite false but it's for the sake of the example)
  - Since many computer science experiments are based on time
    measurements, we focus on *continuous* variables
- \textbf{Note:} To distinguish random variables, which are complex
  objects, from other mathematical objects, they will always be
  written in blue capital letters in this set of slides (\eg \rv{X})

- The probability measure on \Omega induces probabilities on the
  *values* of \rv{X}
  - $\P(\rv{X}=0.5213)$ is generally 0 as the outcome never exactly matches
  - $\P(0.5213\leq\rv{X}\leq0.5214)$ may however be non-zero
*** Probability distribution
#+begin_src R :results output graphics :file "pdf_babel/Gamma_distribution.pdf" :exports none :width 6 :height 3 :session
library(ggplot2)
library(ggthemes)
df = data.frame(x=c(-2,10), y=c(0,.3))
func = dgamma
pfunc = pgamma
args = list(shape = 3)
xmin = 1
xmax = 6
x = seq(from=xmin,to=xmax,length.out=50)
y = do.call(func,c(list(x=x),args))
area = data.frame(x=x, y=y)
integral = diff(range(do.call(pfunc,c(list(q=c(xmin,xmax)),args))))
label = paste("P(paste(",xmin," <= X) <= ",xmax,") == ", integral)

r2.value <- 0.90


p = ggplot(data=df,aes(x=x,y=y)) + geom_point(size=0) + theme_classic() + 
    stat_function(fun = func, colour = "darkgreen", arg = args) +
        geom_area(data=area,aes(x=x,y=y),fill="lightskyblue2") + 
            geom_text(x=.7*max(df$x),y=.7*max(df$y), label=label, parse=T) +
            ylab(expression(paste(f[X],"(",italic(w),")"))) + xlim(df$x) +
            xlab(expression(italic(w)))
p
# ggsave(p,file="pdf_babel/Gamma_distribution.pdf",width=6,height=4)
#+end_src

#+RESULTS:
[[file:pdf_babel/Gamma_distribution.pdf]]

A *probability distribution* (a.k.a. *probability density function* or
p.d.f.) is used to describe the probabilities of different *values*
occurring

- A random variable \rv{X} has density $f_X$, where $f_X$ is a
  non-negative and integrable function, if:
  #+BEGIN_LaTeX
  \vspace{-.6em}
  \begin{equation*}
  \P[a \leq \rv{X} \leq b] = \int_a^b f_X(w) \, \dif w
  \end{equation*}\vspace{-.8em}
  #+END_LaTeX
  #+BEGIN_CENTER
  \includegraphics[width=.7\linewidth]{pdf_babel/Gamma_distribution.pdf}
  #+END_CENTER
  \vspace{-.7em}
- \textbf{Note}: people often confuse the sample space with the random
  variable. Try to make the difference when modeling your system, it
  will help you
- \small Note: /the/ $\boxed{\text{X}}$ /in/ $\boxed{1\leq\text{X}\leq6}$ /in
  the above figure should be in blue.../
*** Characterizing a random variable
The probability density function *fully characterizes* the random
variable but it is also complex object

- It may be symmetrical or not
- It may have one or several *modes*
- It may have a bounded support or not, hence the random variable may
  have a *minimal* and/or a *maximal* value
- The *median* cuts the probabilities in half

#+begin_src R :results output graphics :file "pdf_babel/distribution_characteristics.pdf" :exports none :width 6 :height 3 :session
library(ggplot2)
library(ggthemes)
xmin = -2
xmax = 10
ymin = 0
ymax = .3

func = dgamma
pfunc = pgamma
args = list(shape = 3)

x = seq(from=xmin,to=xmax,length.out=500)
y = do.call(pfunc,c(list(q=x),args))
dfminx = data.frame(x=x,y=y)
minx = tail(dfminx[dfminx$y==0,],n=1)$x
if(length(minx)==0) {minx=NA}
maxx = head(dfminx[dfminx$y==1,],n=1)$x
if(length(maxx)==0) {maxx=NA}
medianx = tail(dfminx[dfminx$y<.5,],n=1)$x
if(length(medianx)==0) {medianx=NA}

y = do.call(func,c(list(x=x),args))
dfminx = data.frame(x=x,y=y)
modex = dfminx[dfminx$y==max(dfminx$y),]$x
espx = sum(dfminx$x*dfminx$y)*diff(range(head(dfminx$x,n=2)))

dfstat = data.frame(name = c("min", "median", "max","mode","expected value"),
                    x = c(minx,medianx,maxx,modex,espx),
                    y = ymax)

p = ggplot(data=dfstat,aes(x=x,y=y,color=name)) + geom_line(alpha=1) +
    xlim(xmin,xmax) + ylim(ymin,ymax) + theme_classic() + ylab("f(x)") +
    stat_function(fun = func, colour = "black", arg = args) +
    geom_vline(aes(xintercept=x,color=name)) + guides(colour = guide_legend(""))

p
# ggsave(p,file="pdf_babel/Gamma_distribution.pdf",width=6,height=4)
#+end_src

#+RESULTS:
[[file:pdf_babel/distribution_characteristics.pdf]]

#+BEGIN_CENTER
\includegraphics[width=.8\linewidth]{pdf_babel/distribution_characteristics.pdf}
#+END_CENTER
*** Expected value
- When one speaks of the "expected price", "expected height", etc. one
  means the *expected value* of a random variable that is a price, a
  height, etc.
  \begin{align*}
  \E[\rv{X}] & = x_1p_1 + x_2p_2 + \ldots + x_kp_k = \int_{-\infty}^\infty x f(x)\, \dif x
  \end{align*}
  The expected value of \rv{X} is the ``average value'' of \rv{X}.\smallskip

  It is \textbf{not} the most probable value. The mean is _one_ aspect
  of the distribution of \rv{X}. The *median* or the *mode* are other
  interesting aspects.
- The *variance* is a measure of how far the values of a
  random variable are spread out from each other.

  If a random variable \rv{X} has the expected value (mean) $\mu =
  \E[\rv{X}]$, then the variance of \rv{X} is given by:
  #+BEGIN_LaTeX
  \begin{align*} 
      \Var(\rv{X}) &= \E\left[(\rv{X} - \mu)^2
      \right] = \int_{-\infty}^\infty  (x-\mu)^2 f(x)\, \dif x
  \end{align*}
  #+END_LaTeX
- The *standard deviation $\sigma$* is the square root of the variance. This
  normalization allows to compare it with the expected value
* Using the model to make Estimations
** Estimating the Expected value
*** How to estimate the Expected value ?
To empirically *estimate* the expected value of a random variable
\rv{X}, one repeatedly measures observations of the variable and
computes the arithmetic mean of the results \bigskip 

This is called the *sample mean* \bigskip 

Unfortunately, if you repeat the estimation, you may get a different
value since \rv{X} is a random variable \dots
*** Central Limit Theorem [\textbf{CLT}]
- Let $\{\rv{X_1}, \rv{X_2}, \dots, \rv{X_n}\}$ be a random sample of size
  $n$ (\ie a sequence of *independent* and *identically distributed*
  random variables with expected values $\mu$ and variances $\sigma^2$)
- The *sample mean* of these random variables is:
  #+BEGIN_LaTeX
  \begin{equation*}
  \rv{S_n} =  \frac{1}{n} (\rv{X_1} + \dots + \rv{X_n})
  \end{equation*}
  #+END_LaTeX
  $\rv{S_n}$ is a random variable too!
- For large n's, the distribution of $\rv{S_n}$ is approximately
  *normal* with *mean $\mu$* and *variance $\frac{\sigma^2}{n}$*
  #+BEGIN_LaTeX
  \begin{equation*}
  \rv{S_n} \xrightarrow[n\to\infty]{} \N\left(\mu,\frac{\sigma^2}{n}\right)
  \end{equation*}
  #+END_LaTeX
*** CLT Illustration
#+begin_src R :results output graphics :file "pdf_babel/CLT_illustration.pdf" :exports none :width 9 :height 6 :session
library(ggplot2)
library(ggthemes)

triangle <- function(n=10) {
  sqrt(runif(n)) 
}

broken <- function(n=10) {
  x=runif(n);
  x/(1-x);
}

broken_mid <- function(n=10) {
  x=(runif(n)+runif(n))/2;
  x/(1-x);
}


generate <- function(n=50000,N=c(1,2,5,10,15,20,30,100), law=c("unif","binom","triangle")) {
  df=data.frame();
  for(l in law) {
    for(p in N) {
      X=rep.int(0,n);
      for(i in 1:p) {
        X = X + switch(l, unif = runif(n),
                          binom = rbinom(n,1,.5), 
                          exp=rexp(n,rate = 2), 
                          norm=rnorm(n,mean = .5),
                          triangle=triangle(n)-1/6,
                          broken=broken(n),
                          broken_mid=broken_mid(n));
      }
      X = X/p;
      df=rbind(df,data.frame(N=p,SN=X,law=l));
    }
  } 
  df;
}
d=generate()
ggplot(data=d,aes(x=SN)) + geom_density(aes(y = ..density..)) + 
     facet_grid(law~N) + theme_classic() + xlab("") + 
     scale_x_continuous(breaks=c(0,.5,1))
#+end_src

#+RESULTS:
[[file:pdf_babel/CLT_illustration.pdf]]

  
Start with an arbitrary distribution and compute the distribution of
$S_n$ for increasing values of $n$.
#+BEGIN_CENTER
#+LaTeX: \includegraphics<1>[width=.8\linewidth]{pdf_babel/CLT_illustration.pdf}
#+END_CENTER
*** The Normal Distribution
#+begin_src R :results output graphics :file "pdf_babel/normal_distribution.pdf" :exports none :width 6 :height 2.5 :session
library(ggplot2)
library(ggthemes)
xmin = -5
xmax = 5
ymin = 0
ymax = 1.5

dfnorm = data.frame(mu=c(0,0,0,-2),sigma2=c(.1,1,5,.5))
dfnorm$label = paste0("mu=",dfnorm$mu,", sigma^2=",dfnorm$sigma2)

df = data.frame(x=c(xmin,xmax),y=c(ymin,ymax))

p = ggplot(data=df,aes(x=x,y=y)) + 
    xlim(xmin,xmax) + ylim(ymin,ymax) + theme_classic() +
    guides(colour = guide_legend("")) + ylab("f(x)")

## Argh, this does not work either. I have to do it "manually". :(
# for(i in 1:dim(dfnorm)[1]) {
#   d = dfnorm[i,]
#   print(d$label)
#   p = p + stat_function(fun = dnorm, 
#                         arg = list(mean=d$mu, sd=d$sigma2), 
#                         aes(color=dfnorm[i,]$label))
# }

p + stat_function(fun = dnorm, arg = list(mean=dfnorm[1,]$mu, sd=sqrt(dfnorm[1,]$sigma2)), aes(color=dfnorm[1,]$label)) +
    stat_function(fun = dnorm, arg = list(mean=dfnorm[2,]$mu, sd=sqrt(dfnorm[2,]$sigma2)), aes(color=dfnorm[2,]$label)) +
    stat_function(fun = dnorm, arg = list(mean=dfnorm[3,]$mu, sd=sqrt(dfnorm[3,]$sigma2)), aes(color=dfnorm[3,]$label)) +
    stat_function(fun = dnorm, arg = list(mean=dfnorm[4,]$mu, sd=sqrt(dfnorm[4,]$sigma2)), aes(color=dfnorm[4,]$label))
#+end_src

#+RESULTS:
[[file:pdf_babel/normal_distribution.pdf]]

#+BEGIN_LaTeX
  \begin{overlayarea}{\linewidth}{4.5cm}
    \begin{center}%
      \includegraphics<1>[height=4.5cm]{pdf_babel/normal_distribution.pdf}%
      \includegraphics<2>[height=4.5cm]{images/Standard_deviation_diagram.pdf}%
    \end{center}
  \end{overlayarea}
  \uncover<1->{The smaller the variance the more ``spiky'' the
    distribution.}
  \uncover<2->{
#+END_LaTeX
- Dark blue is less than one standard deviation from the mean. For the
  normal distribution, this accounts for about 68% of the set.
- Two standard deviations from the mean (medium and dark blue) account
  for about 95%
- Three standard deviations (light, medium, and dark blue) account for
  about 99.7%
#+LaTeX: }

** Evaluating and Comparing Alternatives With Confidence Intervals
*** CLT consequence: confidence interval
#+begin_src R :results output graphics :file pdf_babel/CI_illustration.pdf :exports none :width 5 :height 3 :session
mu = 500
N = 30
n = 40
X = 0
for (i in 1:N) {
    X = X + mu + runif(n, min = -1, max = 1) # Hence var=1/3
}
# so sigma_n = sqrt(1/3)/sqrt(N)
ci = 2*sqrt(1/3)/sqrt(N);

X = X/N

# length(X[X >= 1775.5 & X <= 1776.6])/length(X)

df = data.frame(x = X, y = seq(1:length(X)))
df$valid = 1
df[abs(df$x - mu) > ci, ]$valid = 0
ggplot(df, aes(x = x, y = y, color = factor(valid))) + geom_point() + 
    geom_errorbarh(aes(xmax = x - ci, xmin = x + ci)) + 
    geom_vline(xintercept = mu) + 
    theme_classic() + guides(colour = guide_legend("")) +
    xlim(mu-3*ci,mu+3*ci) + 
    ylab("Trial #") + xlab("Observation: sample mean with \nconfidence interval") +
    coord_flip() + ggtitle(paste(n," observations of the mean of ",N," samples"))
#+end_src

#+RESULTS:
[[file:pdf_babel/CI_illustration.pdf]]

#+BEGIN_LaTeX
\begin{overlayarea}{\linewidth}{4.5cm}
  \begin{center}%
    \includegraphics<1>[height=4.5cm]{images/Standard_deviation_diagram.pdf}%
    \includegraphics<2>[height=4.5cm]{pdf_babel/CI_illustration.pdf}%
  \end{center}
\end{overlayarea}
#+END_LaTeX

When $n$ is large:
#+BEGIN_LaTeX
\begin{center}
  \scalebox{.9}{$\displaystyle
  \P\left(\mu\in
    \left[\rv{S_n}-2\frac{\sigma}{\sqrt{n}},\rv{S_n}+2\frac{\sigma}{\sqrt{n}}\right]\right)
  = \P\left(\rv{S_n}\in
    \left[\mu-2\frac{\sigma}{\sqrt{n}},\mu+2\frac{\sigma}{\sqrt{n}}\right]\right)
  \approx  95\%$}
\end{center}
\uncover<2>{There is 95\% of chance that the \alert{true mean} lies
  within 2$\frac{\sigma}{\sqrt{n}}$ of the \alert{sample mean}.}
#+END_LaTeX
*** Without any particular hypothesis
- Assume, you have evaluated two *alternatives* $A$ and $B$ on $n$
  different *setups*

- You therefore consider the associated random variables \rv{A} and
  \rv{B} and try to estimate there expected values $\mu_A$ and $\mu_B$
#+BEGIN_LaTeX
  \begin{center}
    \begin{overlayarea}{.9\linewidth}{4.5cm}
      \begin{center}%
        \includegraphics<1>[scale=.911,subfig=1]{fig/2sample_comp.fig}%
        \includegraphics<2>[scale=.911,subfig=2]{fig/2sample_comp.fig}%
        \includegraphics<3->[scale=.911,subfig=3]{fig/2sample_comp.fig}%
      \end{center}
    \end{overlayarea}
  \end{center}
  \vspace{-.8em}
    \begin{overlayarea}{\linewidth}{1.5cm}%
      \only<1>{The two 95\% confidence intervals do not overlap\vspace{-.8em}
        \begin{flushright}
          $\leadsto \mu_A<\mu_B$ with more than 90\% of confidence
          \smiley
        \end{flushright}
      }%
      \only<2>{The two 95\% confidence intervals do overlap\vspace{-.8em}
        \begin{flushright}
          $\leadsto$ Nothing can be concluded \frowny\\
          Reduce C.I ?
        \end{flushright}
      }%
      \only<3>{The two 70\% confidence intervals do not overlap\vspace{-.8em}
        \begin{flushright}
          $\leadsto\mu_A<\mu_B$ with less than 50\% of confidence \frowny
          $\leadsto$ more experiments...
        \end{flushright}
      }%
      \only<4->{The width of the confidence interval is proportional
        to $\frac{\sigma}{\sqrt{n}}$\vspace{-.8em}
        \begin{flushright}
          Halving C.I. requires 4 times more experiments! \frowny\\
          Try to \alert{reduce variance} if you can...\smiley
        \end{flushright}
      }
    \end{overlayarea}
#+END_LaTeX
*** Exploiting blocks
#+BEGIN_LaTeX
\begin{itemize}
\item C.I.s overlap because variance is large. Some *setups* may have an
  intrinsically longer duration than others, hence a large
  $\Var(\rv{A})$ and $\Var(\rv{B})$
  \begin{center}
    \includegraphics<1>[scale=.7,subfig=2]{fig/2sample_comp.fig}%
    \includegraphics<2->[scale=.7,subfig=4]{fig/2sample_comp.fig}%
  \end{center}
\item<2-> The previous test estimates $\mu_A$ and $\mu_B$
  \alert{independently}.

  $\E[\rv{A}]<\E[\rv{B}] \Leftrightarrow \E[\rv{B-A}]>0$.

  In the previous evaluation, the \alert{same} setup $i$ is used for
  measuring $\rv{A_i}$ and $\rv{B_i}$, hence we can focus on $\rv{B-A}$.

  Since $\Var(\rv{B-A})$ is much smaller than $\Var(\rv{A})$ and
  $\Var(\rv{B})$, we can conclude that $\mu_A<\mu_B$ with 95\% of
  confidence.
\item<3-> Relying on such common points is called \alert{blocking}
  and enable to \alert{reduce variance}.
\end{itemize}

#+END_LaTeX
*** Let's reuse a previous example
#+begin_src R :results output graphics :file pdf_babel/comparing_2_alternatives.pdf :exports none :width 5 :height 3 :session
library(ggplot2)
library(dplyr)
library(tidyr)
set.seed(42);
n = 40;
setup_val=rgamma(n,shape=3)
a = setup_val + 1.5 + runif(n)
b = setup_val + 2.5 + runif(n)
diff = .65
df = data.frame(A=a,B=b)
write.csv(df,file="data/set1.csv",row.names=FALSE);
df$Diff=df$B-df$A
dfgg = df %>% gather(Alternative, Time)
dfsummary = dfgg %>% group_by(Alternative) %>%
       summarise(num = n(),
                 mean = mean(Time),
                 sd = sd(Time),
                 ci70 = sd(Time)/sqrt(n()),
                 ci95 = 2*sd(Time)/sqrt(n()),
                 ci99 = 3*sd(Time)/sqrt(n()))
dfsummary = dfsummary %>% gather(CI,ci,ci70,ci95,ci99) 

ggplot(dfgg,aes(x=Alternative,y=Time)) + theme_bw() +
     scale_color_brewer(palette="Set1") +
    guides(fill = "none") + 
    geom_jitter(alpha=.2,position = position_jitter(width = .1)) +
    geom_errorbar(data=dfsummary,width=.3,
                  aes(y=mean,ymin=mean-ci,ymax=mean+ci,color=CI),
                  position="dodge") +
    geom_point(data=dfsummary,shape=21, position="dodge", 
               aes(y=mean,color=CI)) + 
    geom_rect(data=dfsummary[dfsummary$Alternative=="A" &
                             dfsummary$CI=="ci95",], 
              aes(y=mean, xmin="A",xmax="B",
                  ymin=mean-ci,ymax=mean+ci),fill="blue",alpha=.3) +
    geom_hline(yintercept=diff,color="azure4") + 
    annotate("text",x="B",y=1.7*diff,label=diff,size=4,color="azure4")

#+end_src

#+RESULTS:
[[file:pdf_babel/comparing_2_alternatives.pdf]]

#+BEGIN_CENTER
\includegraphics[width=.7\linewidth]{pdf_babel/comparing_2_alternatives.pdf}
#+END_CENTER

\vspace{-.8em} $\mu_A$ is 0.65 seconds smaller than $\mu_B$ with more than
99\% of confidence \smiley\medskip

You need to invest in a probabilistic model. Here we assumed:
#+LaTeX: \begin{columns}\begin{column}{.3\linewidth}
- $\rv{A_i} = \boxed{\rv{S_i}} + \rv{A'_i}$
- $\rv{B_i} = \boxed{\rv{S_i}} + \rv{B'_i}$
#+LaTeX: \end{column}\begin{column}{.7\linewidth}~\\[-.6em]
So we could subtract them \smiley \\
Dividing them would have been a very bad idea...\frowny
#+LaTeX: \end{column}\end{columns}
*** How to compute and plot CI in R: code
\small
#+begin_src R :results output graphics :file pdf_babel/comparing_2_alternatives2.pdf :exports code :width 5 :height 3 :session
library(ggplot2)
library(dplyr)
library(tidyr)
df = read.csv("data/set1.csv",header=T)
df$Diff=df$B-df$A # Assuming observations are paired!
dfgg = df %>% gather(Alternative, Time) 
dfsum = dfgg %>% 
       group_by(Alternative) %>%
       summarise(num = n(), mean = mean(Time), sd = sd(Time),
                 se = 2*sd/sqrt(num))
ggplot(dfgg,aes(x=Alternative,y=Time,color=Alternative)) + 
     scale_color_brewer(palette="Set1") + theme_bw() +
     geom_jitter(alpha=.2,position = position_jitter(width = .1)) +
     geom_errorbar(data=dfsum,width=.2,
                   aes(y=mean,ymin=mean-se,ymax=mean+se),
                   position="dodge") +
     geom_point(data=dfsum,shape=21, position="dodge", size=3,
                aes(y=mean,color=Alternative))
#+end_src

#+RESULTS:
[[file:pdf_babel/comparing_2_alternatives2.pdf]]
*** How to compute and plot CI in R: output
[[file:pdf_babel/comparing_2_alternatives2.pdf]]
** What should I take care of?
*** CLT hypothesis
#+begin_src R :results output graphics :file pdf_babel/sample_var.pdf :exports none :width 5 :height 3 :session
set.seed(42)
df = data.frame(row.names = c("n", "sample_mean", "sample_var"))
for (n in seq(2, 32)) {
    for (N in 1:30) {
        x = rnorm(n, 5, 1)
        df = rbind(df, data.frame(n = c(n), sample_mean = mean(x), sample_var = var(x)))
    }
}
ggplot(df, aes(x = n, y = sample_var)) + theme_classic() +
    geom_point(alpha=.15) + ylab('Samples of "sample variance"') +
    ggtitle("Each dot is the sample variance of n values") +
    geom_hline(yintercept = 1,color="darkred") + 
    annotate(geom="text",label="True variance", x = 25, y = 1.5, 
             color="darkred")
        
#+end_src

#+RESULTS:
[[file:pdf_babel/sample_var.pdf]]

\null\vspace{-2em}
- The CLT hypothesis are very weak: it *does not assume any particular
  distribution* (\eg normality) for \rv{X}

  But, the CLT says that /when $n$ goes large/, the sample mean is
  /normally distributed/. We have seen it holds true quickly\\[0pt]
  #+BEGIN_LaTeX
  \begin{overlayarea}{\linewidth}{0cm}
    \vspace{-1em}
    \begin{center}
      \includegraphics<1>[width=.7\linewidth]{pdf_babel/CLT_illustration.pdf}
    \end{center}
  \end{overlayarea}
  \vspace{-2.4em}
  #+END_LaTeX
- 
  #+BEGIN_LaTeX
  \uncover<2->{%
    However, the CLT uses $\sigma = \sqrt{\Var(X)}$ but we only have the
    \alert{sample variance}, not the \alert{true variance}
  }

  \uncover<3>{%
    So you should always try to either find an \alert{upper bound on the
      true variance} or \alert{overestimate the sample variance}\bigskip
  }

  \begin{overlayarea}{\linewidth}{4cm}
    \vspace{-1em}
    \begin{center}
      \includegraphics<2->[width=.7\linewidth]{pdf_babel/sample_var.pdf}
    \end{center}
  \end{overlayarea}
#+END_LaTeX

*** How many replicates ?

#+BEGIN_LaTeX
\small
\begin{itemize}
\item<+-> 
  \uncover<.->{\textbf{Q:} How Many Replicates ?} \\
  ~\hfill\uncover<+->{\textbf{A1:} How many can you afford ?}\\
  ~\hfill\uncover<+->{\textbf{A2:} 30\dots\hspace{3.25cm}~\\
    \textbf{Rule of thumb:} a sample of 30 or more is big sample but a
    sample of 30 or less is a small one (\uline{doesn't always work})}

\item<+-> With less than 30, you should make the C.I. wider using
  \eg the \alert{Student law}.

\item<+-> Once you have a first C.I. with 30 samples, you can estimate
  how many samples will be required to answer your question. If it is
  too large, then either try to reduce variance (or the scope of your
  experiments) or simply explain that the two alternatives are hardly
  distinguishable... You need a \alert{sequential approach}.

\item<+-> \textbf{Running the right number of experiments enables to
    get to conclusions more quickly and hence to test other
    hypothesis.}
\end{itemize}
#+END_LaTeX
*** Key Hypothesis
  The hypothesis of CLT are very weak. Yet, to qualify as replicates,
  the repeated measurements:
  - must be *independent* (take care of warm-up)
  - must *not* be part of a *time series* (the system behavior may
    temporary change)
  - must *not* come *from the same place* (the machine may have a problem)
  - must be of appropriate *spatial scale*

    #+BEGIN_CENTER
    \textbf{Perform graphical checks}
    #+END_CENTER
*** Simple Graphical Checks                                        :noexport:
#+begin_src R :results output :session :exports both
# From http://rpubs.com/sinhrks/plot_tsstats
library(devtools)
install_github('sinhrks/ggfortify')
#+end_src

#+begin_src R :results output :session :exports both
library(gridExtra)
library(ggplot2)
# library(ggfortify)

four_plot = function (df) {
    p1 = ggplot(df,aes(x=Start,y=Value)) + geom_line() + 
         theme_bw() + ggtitle("Sequence Plot");
    X = df$Value
    df_lag = data.frame(x1=X[1:length(X)-1],x2=X[2:length(X)])
    p2 = ggplot(df_lag,aes(x=x1,y=x2)) + geom_point(alpha=.4) + 
         theme_bw() + ggtitle("Lag Plot") + xlab("Value[i]") + 
         ylab("Value[i+1]");;
    # Or alternatively, if ggfortify is installed, give a try 
    # to gglagplot...
    p3 = ggplot(df,aes(x=Value)) + geom_histogram() +
         theme_bw() + ggtitle("Histogram");
    p4 = ggplot(df,aes(sample=Value)) + stat_qq() + 
         theme_bw() + ggtitle("Normal Probability Plot");
    grid.arrange(p1,p2,p3,p4,nrow=2);
}
#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file pdf_babel/4plot1.pdf :exports both :width 6 :height 4 :session
n = 200
df1 = data.frame(Start=1:n, Value=10+rnorm(n))
four_plot(df1)
#+end_src
#+RESULTS:
[[file:pdf_babel/4plot1.pdf]]

#+begin_src R :results output graphics :file pdf_babel/4plot2.pdf :exports both :width 6 :height 4 :session
n = 1000
df1 = data.frame(Start=1:n, Value=10+rnorm(n))
four_plot(df1)
#+end_src

#+RESULTS:
[[file:pdf_babel/4plot2.pdf]]

#+begin_src R :results output graphics :file pdf_babel/4plot3.pdf :exports both :width 6 :height 4 :session
n = 500
Start=1:n
df1 = data.frame(Start=Start, Value=sin(Start)+.15*rnorm(n))

four_plot(df1)
#+end_src

#+RESULTS:
[[file:pdf_babel/4plot3.pdf]]


#+begin_src R :results output graphics :file pdf_babel/4plot3bis.pdf :exports both :width 6 :height 4 :session
n = 500
Start=1:n

four_plot(head(df1,n=100))
#+end_src

#+RESULTS:
[[file:pdf_babel/4plot3bis.pdf]]


#+begin_src R :results output graphics :file pdf_babel/4plot4.pdf :exports both :width 6 :height 4 :session
n = 500
Start=1:n
df1 = data.frame(Start=Start, Value=20+4*sin(.01*Start)+1*rnorm(n)*4*sin(.01*Start))

four_plot(df1)
#+end_src

#+RESULTS:
[[file:pdf_babel/4plot4.pdf]]

#+begin_src R :results output graphics :file pdf_babel/4plot5.pdf :exports both :width 6 :height 4 :session
n = 500
Start=1:n
df1 = data.frame(Start=Start, Value=rgamma(n,shape=.5))

four_plot(df1)
#+end_src

#+RESULTS:
[[file:pdf_babel/4plot5.pdf]]

*** Simple Graphical Checks
#+BEGIN_LaTeX
  \vspace{-.8em}
  \begin{center}
    \includegraphics<1>[height=5cm]{pdf_babel/4plot1.pdf}%
    \includegraphics<2>[height=5cm]{pdf_babel/4plot2.pdf}%
    \includegraphics<3>[height=5cm]{pdf_babel/4plot3.pdf}%
    \includegraphics<4>[height=5cm]{pdf_babel/4plot3bis.pdf}%
    \includegraphics<5>[height=5cm]{pdf_babel/4plot4.pdf}%
    \includegraphics<6>[height=5cm]{pdf_babel/4plot5.pdf}%
  \end{center}
  \vspace{-.8em}
  %  \hspace{-.05\linewidth}
  \begin{minipage}{1\linewidth}
    \small
#+END_LaTeX
- Fixed Location :: the run sequence plot should be _flat_ and
                    _non-drifting_\vspace{-.5em}
- Fixed Variation ::  the vertical _spread_ in the run sequence plot
     should _approximately the same_ over the entire horizontal
     axis\vspace{-.5em}
- Independence :: the lag plot should be _structureless_\vspace{-.5em}
- Fixed Distribution :: (, in particular if the /fixed normal
     distribution/ assumption holds)\\[-1.7em]
  - the histogram should be bell-shaped, and
  - the normal probability plot should be linear \vspace{-.9em}
If you see _several modes_, there may be an _hidden parameter_ to take
into account
#+LaTeX: \end{minipage}
*** Temporal Dependancy

#+begin_src R :results output graphics :file pdf_babel/deptempo.pdf :exports results :width 6 :height 4 :session
n = 1000
Start=1:n
df1 = data.frame(Start=Start, Value=.5*sin(.1*Start)+.5*rnorm(n))
#df1 = data.frame(Start=Start, Value=20+4*sin(.01*Start)+1*rnorm(n)*4*sin(.01*Start))

p1 = ggplot(df1,aes(x=Start,y=Value)) + geom_line() + ggtitle("Sequence Plot")
p2 = ggplot(head(df1,n=100),aes(x=Start,y=Value)) + geom_line() + ggtitle("Sequence Plot (zoom)")
grid.arrange(p1,p2,nrow=1)
#+end_src

#+RESULTS:
[[file:pdf_babel/deptempo.pdf]]

- Looks independant and statistically identical
- *Danger*: temporal correlation $\leadsto$ study *stationnarity*
- *Periodicity* : May depend on sampling frequency or on horloge
  resolution.
  - Study the period (Fourier), use time series
*** Detect Trends
#+begin_src R :results output graphics :file pdf_babel/unifderiv.pdf :exports results :width 6 :height 4 :session
n = 500
Start=1:n
df1 = data.frame(Start=Start, Value=1+atan(Start/70)*(runif(n,min=.3,max=1)))

ggplot(df1,aes(x=Start,y=Value)) + geom_line() + ggtitle("Sequence Plot")
#+end_src

#+RESULTS:
[[file:pdf_babel/unifderiv.pdf]]

- Model the trend: here increase then saturates
- Possibly remove the trend by compensating it (multiplicative factor
  here) or removing what can be identified as a _warm-up_
*** Confidence...
#+BEGIN_LaTeX
\begin{center}
  \begin{tabular}{c|c}
    \includegraphics[scale=.32]{images/xkcd_significant_1.png}&
    \includegraphics[scale=.32]{images/xkcd_significant_2.png}
  \end{tabular}
\end{center}
#+END_LaTeX
* Design of Experiments: Early Intuition                           :noexport:
** 
*** Comparing Two Alternatives (Blocking + Randomization)
  \begin{itemize}[<+->]
  \item When comparing A and B for different settings, doing $A, A, A,
    A, A, A$ and then $B, B, B, B, B, B$ is a bad idea.
  \item You should better do $A, B, \quad A, B,\quad A, B,\quad A, B,
    \dots $.
  \item Even better, randomize your run order. You should flip a coin
    for each configuration and start with A on head and with B on
    tail...
    \begin{center}
      $A, B,\quad B, A,\quad  B, A,\quad A, B, \dots $.
    \end{center}
    With such design, you will even be able to check whether being the
    first alternative to run changes something or not.
  \item Each configuration you test should be run on different
    machines.
    
    You should record as much information as you can on how the
    experiments was performed (\url{http://expo.gforge.inria.fr/}).
  \end{itemize}

*** Experimental Design
There are two key concepts:
#+BEGIN_CENTER
  *replication* and *randomization*
#+END_CENTER
You replicate to *increase reliability*. You randomize to *reduce bias*.
#+BEGIN_CENTER
    \textbf{If you replicate thoroughly and randomize properly, \\ you will not go far wrong.}  
#+END_CENTER
\pause
#+BEGIN_QUOTE
  \sf
  It doesn't matter if you cannot do your own advanced statistical
  analysis. If you designed your experiments properly, you may be able
  to find somebody to help you with the statistics.\smallskip

  If your experiments is not properly designed, then no matter how
  good you are at statistics, you experimental effort will have been
  wasted.
#+END_QUOTE
\vspace{-1em}
#+BEGIN_CENTER
  \textbf{No amount of high-powered statistical analysis can turn a bad experiment into a good one.}
#+END_CENTER

*** TO DO
Experimental Design (replication and randomization)
- general recommendations (parsimony, true randomization)
- replication vs. pseudo-replication
- Strong inference vs. weak inference

* Getting rid of Outliers                                          :noexport:
** Abnormal measurements
  \begin{center}
    \includegraphics<1>[width=\linewidth,height=3cm]{simul/cauchy1.pdf}%
  \end{center}
  \begin{itemize}
  \item Rare events: interpretation
  \item Get rid of it using:
    \begin{itemize}
    \item a threshold value: what is the right threshold ?
    \item quantiles: what is the good rejection rate ?
    \end{itemize}
  \end{itemize}

** Thresholds
  \begin{center}
    Reject values larger than 10 $\leadsto$ 5\% of rejection\\
    \includegraphics<1>[width=\linewidth,height=3cm]{simul/cauchy-seuil10.pdf}%
  \end{center}


  \begin{center}
    Reject values larger than 50 $\leadsto$ 1\% of rejection\\
    \includegraphics<1>[width=\linewidth,height=3cm]{simul/cauchy-seuil1pc.pdf}%
  \end{center}

  Actually, here, the samples are generated using the Cauchy
  distribution, which is pathological for most ideas you may come up
  with. :)

* Issues when studying something else than the mean                :noexport:
** Summarizing the distribution
  \begin{center}
    \includegraphics[width=\linewidth,height=3cm]{simul/histogramme2.pdf}%
  \end{center}
  What is the shape of the histogram:
  \begin{itemize}
  \item uni/multi-modal
  \item symmetrical or not ($\leadsto$ skewness)
  \item Flat of not ($\leadsto$ kurtosis)
  \end{itemize}
  Summarize with \alert{central tendancy}

** Summarizing the distribution
  \begin{center}
    \includegraphics[width=\linewidth,height=3cm]{simul/histogramme2.pdf}%
  \end{center}
  \begin{itemize}
  \item Mode: the most probable value (higly depends on the
    bin size)
  \item Median: splits the samples in half (rather unstable)
  \item Mean: average ``cost'' (can simply estimate confidence intervals)
  \end{itemize}

** Mode value
{\bf histogram}
\begin{center}
\includegraphics[width=4.6cm]{simul/histogramme.pdf}
\end{center}
\begin{block}{Mode}
\begin{itemize}
\item  {\green{\bf Categorical data}}
\item Most frequent value
\item highly unstable value
\item for continuous value distribution depends on the histogram step
\item interpretation depends on the flatness of the histogram
\end{itemize}
\alert{\bf $\Longrightarrow$ Use it carefully} 

\alert{\bf $\Longrightarrow$ Predictor function} 

\end{block}

** Median value
{\bf histogram}
\begin{center}
%\includegraphics[width=4.6cm]{histogramme.pdf}
\end{center}
\begin{block}{Median}
\begin{itemize}
\item {\green{\bf Ordered data}}
\item Split the sample in two equal parts 
\[
\sum_{i\leq Median}f_i \leq \frac 1 2 \leq \sum_{i\leq Median+1}f_i .\]
\item more stable value
\item does not depends on the histogram step 
\item difficult to combine (two samples)
\end{itemize}


\alert{\bf $\Longrightarrow$ Randomized algorithms} 

\end{block}

** Mean value
{\bf histogram}
\begin{center}
%\includegraphics[width=4.6cm]{histogramme.pdf}
\end{center}
\begin{block}{Mean}
\begin{itemize}
\item {\green{\bf Vector space}}
\item Average of values 
\[
Mean = \frac 1 {Sample\_Size}\sum x_i = \sum_{x}x.f_x .\]
\item stable value 
\item does not depends on the histogram step 
\item easy to combine (two samples $\Rightarrow$ weighted mean)
\end{itemize}


\alert{\bf $\Longrightarrow$ Additive problems (cost, durations, length,...)} 

\end{block}

** Central tendency
{\bf histogram}
\begin{center}
\includegraphics[width=4.6cm]{simul/histogramme2.pdf}
\end{center}
\begin{block}{Complementarity}
\begin{itemize}
\item Valid if the sample is "Well-formed"
\item {\green{\bf Semantic of the observation}}
\item Goal of analysis 
\end{itemize}

\alert{\bf $\Longrightarrow$ Additive problems (cost, durations, length,...)} 

\end{block}

** Central tendency (2)
\begin{block}{Summary of Means}
\begin{itemize}
\item Avoid means if possible\\
Loses information
\item  {\blue Arithmetic mean}\\
 When sum of raw values has physical meaning\\
Use for summarizing times (not rates)
\item {\blue Harmonic mean}\\
Use for summarizing rates (not times)
\item {\blue Geometric mean}\\
 Not useful when time is best measure of perf\\
 Useful when multiplicative effects are in play
\end{itemize}
\end{block}

** Computational aspects
\begin{itemize}
\item Mode : computation of the histogram steps, then computation of max  $O(n)$  ``off-line''
\item Median : sort the sample $O(nlog(n))$ or $O(n)$ (subtile algorithm) ``off-line''
\item Mean : sum values $O(n)$ ``on-line'' computation
\end{itemize}
\pause
\begin{center}
\alert{\large \bf Is the central tendency significant ? \\
$\Rightarrow$ Explain variability.}
\end{center}

** Variability
\begin{block}{Categorical data (finite set)}
$f_i$ : empirical frequency of element $i$

Empirical entropy
\[
H(f)=\sum_i f_i \log f_i.\]
Measure the empirical distance with the uniform distribution
\begin{itemize}
\item $H(f)\geq 0$
\item $H(f)=0$ iff the observations are reduced to a unique value
\item $H(f)$ is maximal for the uniform distribution
\end{itemize} 
\end{block}
\begin{frame}{Variability (2)}
\begin{block}{Ordered data }

Quantiles : quartiles, deciles, etc

Sort the sample : 
\[
(x_1,x_2,\cdots ,x_n)\longrightarrow  (x_{(1)},x_{(2)},\cdots ,x_{(n)});\]
\[
Q_1=x_{(n/4)};\; Q_2=x_{(n/2)}=Median;\; Q_3=x_{(3n/4)}.\]

For deciles
\[
d_i = argmax_i \{\sum_{j\leq i}f_j \leq \frac i {10}\}.\]
Utilization as quantile/quantile plots to compare distributions
\end{block}
\begin{frame}{Variability (3)}
\begin{block}{ Vectorial data }

Quadratic error for the mean 
\[
Var(X)=\frac 1 n \sum_1^n (x_i-\bar{x}_n)^2.\]
{\bf Properties:}
\begin{eqnarray*}
Var(X) & \geq & 0;\\
Var(X)&=&\overline{x^2}-(\bar{x})^2, \;\;\mbox{o\`u} \;\;\overline{x^2}=\frac 1 n \sum_{i=1}^n x_i^2.\;\\
Var(X+cste)&=&Var(X);\\
Var(\lambda X)&=&\lambda^2 Var(X).
\end{eqnarray*}

\end{block}

** Roadmap for a good data analysis
\begin{enumerate}
\item Plot the sample (various representations)
\item Describe the results (data analysis)
\item Preliminary processing : remove or flag outliers, estimate or flag  missing values
\item Propose a stochastic model : establish the hypothesis : independence (time correlation, auto-correlation), stationarity, same probability law
\item Summarize data by a  histogram
\item Comment the shape (modal/skewness/flatness/...)
\item Estimate the central tendency of the sample : choose the central index
\item Estimate the accuracy of the result (confidence intervals)
\item Propose a visualization
\end{enumerate}

** References
   \nocite{Jain1991,Lilja200912,Ross201002,Montgomery200901}
   \bibliographystyle{plain}
   \bibliography{biblio}


